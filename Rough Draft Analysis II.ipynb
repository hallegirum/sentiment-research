{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough Draft- Text Analysis Project \n",
    "Halleluiah Girum\n",
    "April 4, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the articles \n",
    "df_news = pd.read_excel(r\"C:\\Users\\Halle\\Documents\\Econ1680\\project-2-hallegirum\\news_data.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix using word count \n",
    "# assign stop words \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union('saudi arabia')\n",
    "stop_words = list(stop_words)\n",
    "# use word count method to create a document-term matrix \n",
    "wordCount = CountVectorizer(stop_words=stop_words)\n",
    "X_wordcount = wordCount.fit_transform(df_news['Abstract'])\n",
    "features = wordCount.get_feature_names_out()\n",
    "\n",
    "# changing the matrix into a dataframe \n",
    "\n",
    "X_wordcount = X_wordcount.toarray()\n",
    "X_df = pd.DataFrame(X_wordcount,columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent bi-grams with word count method \n",
    "# assign stop words \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union('saudi arabia')\n",
    "stop_words = list(stop_words)\n",
    "new_words = [\"saudi\",\"arabia\", \"iran\", \"iraq\", \"middle\", \"east\",\"united\",\"states\",\"persian\" ,\"gulf\",\"gulf\", \"states\",\"arab\"]\n",
    "stop_words.extend(new_words)\n",
    "vect = CountVectorizer(ngram_range=(2,2),stop_words= stop_words)\n",
    "word_count = vect.fit_transform(df_news['Abstract'])\n",
    "word_count = word_count.toarray()\n",
    "vocab = vect.get_feature_names_out()\n",
    "\n",
    "word_count_df = pd.DataFrame(word_count,columns=vocab)\n",
    "\n",
    "# get the term document matrix in order to get most frequent words\n",
    "term_doc = word_count_df.T\n",
    "term_doc['total count'] = term_doc.sum(axis=1)\n",
    "term_doc = term_doc.sort_values(by='total count',ascending=False)\n",
    "\n",
    "print(term_doc[0:20])\n",
    "\n",
    "term_doc[0:20]['total count'].plot.bar()\n",
    "plt.title ('Most Common Bigrams: Word Count')\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Frequency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent bi-grams with TFIDF method \n",
    "# assiging stop words, adding words used in search \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union('saudi arabia')\n",
    "stop_words = list(stop_words)\n",
    "new_words = [\"saudi\",\"arabia\", \"iran\", \"iraq\", \"middle\", \"east\",\"united\",\"states\",\"persian\" ,\"gulf\",\"gulf\", \"states\",\"arab\"]\n",
    "stop_words.extend(new_words)\n",
    "# TFIDF coung\n",
    "vect = TfidfVectorizer(ngram_range=(2,2),stop_words=stop_words)\n",
    "word_count = vect.fit_transform(df_news['Abstract'])\n",
    "word_count = word_count.toarray()\n",
    "vocab = vect.get_feature_names_out()\n",
    "\n",
    "word_count_df = pd.DataFrame(word_count,columns=vocab)\n",
    "\n",
    "# get the term document matrix in order to get most frequent words\n",
    "term_doc = word_count_df.T\n",
    "term_doc['total count'] = term_doc.sum(axis=1)\n",
    "term_doc = term_doc.sort_values(by='total count',ascending=False)\n",
    "\n",
    "print(term_doc[0:20])\n",
    "\n",
    "term_doc[0:20]['total count'].plot.bar()\n",
    "plt.title('Most Common Bigrams- TFIDF Method')\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix using TFIDF \n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X_TFIDF = vectorizer.fit_transform(df_news['Abstract'])\n",
    "features2 = vectorizer.get_feature_names_out()\n",
    "\n",
    "# changing the matrix into a dataframe \n",
    "X_TFIDF = X_TFIDF.toarray()\n",
    "X_df2 = pd.DataFrame(X_TFIDF,columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading spending data \n",
    "\n",
    "df_spending = pd.read_excel(r\"C:\\Users\\Halle\\Documents\\Econ1680\\project-2-hallegirum\\spending_data.xlsx\")\n",
    "df_change = pd.read_excel(r\"C:\\Users\\Halle\\Documents\\Econ1680\\project-2-hallegirum\\spending_change.xls\")\n",
    "df_share = pd.read_excel(r\"C:\\Users\\Halle\\Documents\\Econ1680\\project-2-hallegirum\\share_of_gdp.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up data \n",
    "\n",
    "\n",
    "df_change.drop(index=[0,1,2,3,4,5,6,7,8,9],inplace=True)\n",
    "df_change.rename(columns={'Unnamed: 0' : 'Year','Unnamed: 1' : 'Change in Expenditure (from prev year)'},inplace=True )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot summary statistics \n",
    "\n",
    "x = df_change['Year']\n",
    "y = df_change ['Change in Expenditure (from prev year)']\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Change in Expenditure (in %)')\n",
    "plt.title('Change in Defense Expenditure From Previous Year')\n",
    "plt.axhline(y=0,color='black')\n",
    "plt.grid(True,which='both')\n",
    "plt.plot(x,y)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting summary statistics of spending data \n",
    "x = df_share['observation_date']\n",
    "y= df_share['National Defense spending (share of GDP)']\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Spending as Share of GDP (in %)')\n",
    "plt.title('National Defense spending (share of GDP) overtime')\n",
    "plt.plot(x,y)\n",
    "plt.grid(True,which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting summary statistics of spending data \n",
    "x = df_spending['Year']\n",
    "y1 = df_spending['Overseas Contingency Operations Spending ']\n",
    "y2 = df_spending['Army Spending']\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Spending')\n",
    "plt.legend(labels=['Spending on Overseas Operations','Overall Spending on Army'])\n",
    "plt.title('How Military Spending and Overseas Operations Spending Change')\n",
    "plt.grid(True,which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting summary statistcs\n",
    "\n",
    "df_military = pd.merge(df_spending,df_share,on='Year')\n",
    "\n",
    "df_military.dropna(subset=['National Defense spending (share of GDP)'])\n",
    "\n",
    "x= df_military['Year']\n",
    "y1 = (df_military['National Defense spending (share of GDP)'] - df_military['National Defense spending (share of GDP)'].mean())/ df_military['National Defense spending (share of GDP)'].std()\n",
    "y2 = (df_military['Overseas Contingency Operations Spending '] - df_military['Overseas Contingency Operations Spending '].mean()) / df_military['Overseas Contingency Operations Spending '].std()\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Standardized Spending')\n",
    "plt.legend(labels=['National Defense spending (share of GDP)','Spending on Overseas Operations'])\n",
    "plt.title('How Spending as a share of GDP and Overseas Operations Spending Change')\n",
    "plt.grid(True,which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary statistcs of text data \n",
    "\n",
    "# most common publications \n",
    "pubs = {}\n",
    "newspapers = df_news['pubtitle']\n",
    "newspapers.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "for p in newspapers:  \n",
    "    pubs[p] = df_news['pubtitle'].value_counts()[p]\n",
    "\n",
    "        \n",
    "print(pubs)\n",
    "\n",
    "# plotting bar graph \n",
    "pubs_array = []\n",
    "\n",
    "for p in pubs.keys():\n",
    "    pubs_array.append(pubs.get(p))\n",
    "\n",
    "x = pubs.keys()\n",
    "y = pubs_array\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Publications')\n",
    "plt.ylabel('Number of Observations')\n",
    "plt.title('Number of Observations per Publication')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations per time\n",
    "\n",
    "df_news['entryDate'] = pd.to_datetime(df_news['entryDate'],)\n",
    "df_news['Month'] = df_news['entryDate'].dt.month \n",
    "\n",
    "df_news['Quarter'] =0 \n",
    "# # creating a quarter column\n",
    "for i in range(len(df_news)):\n",
    "    if df_news.loc[i,'Month'] <4:\n",
    "        df_news.loc[i,'Quarter']= 1\n",
    "    if df_news.loc[i,'Month']>= 4 and df_news.loc[i,'Month'] <7:\n",
    "        df_news.loc[i,'Quarter'] = 2\n",
    "    if df_news.loc[i,'Month'] >=7 and df_news.loc[i,'Month']<10:\n",
    "        df_news.loc[i,'Quarter'] = 3\n",
    "    if df_news.loc[i,'Month'] >=10:\n",
    "        df_news.loc[i,'Quarter'] =4\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysentiment2 as ps\n",
    "\n",
    "# get the postive/negative scores for all the features \n",
    "h_wordlist = ps.HIV4()\n",
    "\n",
    "scores = []\n",
    "for i in df_news['Abstract']:\n",
    "  tokens = h_wordlist.tokenize(i)\n",
    "  score = h_wordlist.get_score(tokens)\n",
    "  scores.append(score)\n",
    "  \n",
    "scores = pd.DataFrame(scores)\n",
    "df_news['score'] = scores['Polarity']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the seniti word list \n",
    "df_dict = pd.read_csv(r\"C:\\Users\\Halle\\Documents\\Econ1680\\project-2-hallegirum\\word_list.csv\")\n",
    "\n",
    "df_dict = df_dict[['CONCEPT','POLARITY INTENSITY']]\n",
    "\n",
    "features2 = pd.DataFrame(features2)\n",
    "\n",
    "features2.rename(columns={0:'word'},inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words =[]\n",
    "for j in range(len(df_dict)):\n",
    "    if df_dict.loc[j,'POLARITY INTENSITY'] > 0 :\n",
    "        pos_words.append(df_dict.loc[j,'CONCEPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting postive and negative word lists \n",
    "\n",
    "neg_words = []\n",
    "for j in range(len(df_dict)):\n",
    "    if df_dict.loc[j,'POLARITY INTENSITY'] < 0 :\n",
    "        neg_words.append(df_dict.loc[j,'CONCEPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = features2['word'].to_list()\n",
    "\n",
    "pos_score = []\n",
    "neg_score = []\n",
    "doc_score = []\n",
    "\n",
    "# assining a positive and negative score and using the TFIDF vectors in order to scale \n",
    "\n",
    "for i in range (len(df_news)):\n",
    "    p_score = 0 \n",
    "    n_score = 0 \n",
    "    score = 0\n",
    "    doc = df_news['Abstract'][i]\n",
    "    token = doc.split(' ')\n",
    "    for t in token:\n",
    "        if t in words and t in pos_words:\n",
    "            p_score = p_score + (1 * X_df2.loc[i,t])\n",
    "            score = score + (1 * X_df2.loc[i,t])\n",
    "        elif  t in words and t in neg_words:\n",
    "            n_score = n_score + (- 1* X_df2.loc[i,t])\n",
    "            score = score + (-1 * X_df2.loc[i,t])\n",
    "           \n",
    "    pos_score.append(p_score)\n",
    "    neg_score.append(n_score)\n",
    "    doc_score.append(score)\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning a postive and negative score based on word count \n",
    "features = pd.DataFrame(features)\n",
    "\n",
    "features.rename(columns={0:'word'},inplace=True)\n",
    "\n",
    "words = features['word'].to_list()\n",
    "\n",
    "\n",
    "pos_score2 = []\n",
    "neg_score2 = []\n",
    "doc_score2 =[]\n",
    "\n",
    "\n",
    "# assining a positive and negative score and using word count \n",
    "\n",
    "for i in range (len(df_news)):\n",
    "    p_score = 0 \n",
    "    n_score = 0 \n",
    "    d_score = 0\n",
    "    doc = df_news['Abstract'][i]\n",
    "    token = doc.split(' ')\n",
    "    for t in token:\n",
    "        if t in words and t in pos_words:\n",
    "            p_score = p_score + 1\n",
    "            d_score = d_score + 1\n",
    "        elif  t in words and t in neg_words:\n",
    "            n_score = n_score - 1\n",
    "            d_score = d_score -1 \n",
    "           \n",
    "    pos_score2.append(p_score)\n",
    "    neg_score2.append(n_score)\n",
    "    doc_score2.append(d_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['Polarity_wordcount'] = 0\n",
    "\n",
    "for i in range (len(pos_score)):\n",
    "    total = pos_score2[i] - neg_score2[i]\n",
    "    if total !=0:\n",
    "        df_news.loc[i,'Polarity_wordcount'] = (pos_score2[i] + neg_score2[i])/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning polariy scores as an average of the postive and negative scores \n",
    "\n",
    "doc_score = pd.DataFrame(doc_score,index=range(0,785))\n",
    "df_news['Polarity_TFIDF'] = doc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score to df_news data set\n",
    "df_scores = pd.DataFrame(scores,index=range(0,785))\n",
    "\n",
    "df_news['score'] = df_scores['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean scores in each quarter \n",
    "\n",
    "mean_scores = df_news.groupby(['year','Quarter'],as_index=False)['Polarity_wordcount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean scores of each quarter for the polarity using word count \n",
    "\n",
    "x = mean_scores['year']\n",
    "y= mean_scores['Polarity_wordcount']\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Polarity Score')\n",
    "plt.title('Newspaper Sentinment on the Middle East (Word count)')\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "plt.scatter(x,y,c='purple')\n",
    "plt.plot(x,a*x+b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean score using tfidf polarity scores \n",
    "mean_score2 = df_news.groupby(['year','Quarter'],as_index=False)['Polarity_TFIDF'].mean()\n",
    "x = mean_score2['year']\n",
    "y = mean_score2['Polarity_TFIDF']\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Polarity Score')\n",
    "plt.title('Newspaper Sentinment on the Middle East (TFIDF)')\n",
    "\n",
    "a, b = np.polyfit(x, y, 1)\n",
    "plt.scatter(x,y,c='purple')\n",
    "plt.plot(x,a*x+b)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the month and quarter for the spending data \n",
    "\n",
    "\n",
    "df_share['observation_date'] = pd.to_datetime(df_share['observation_date'],)\n",
    "df_share['Month'] = df_share['observation_date'].dt.month \n",
    "df_share['Year'] = df_share['observation_date'].dt.year\n",
    "\n",
    "df_share['Quarter'] =0 \n",
    "# # creating a quarter column\n",
    "for i in range(len(df_share)):\n",
    "    if df_share.loc[i,'Month'] <4:\n",
    "        df_share.loc[i,'Quarter']= 1\n",
    "    if df_share.loc[i,'Month']>= 4 and df_share.loc[i,'Month'] <7:\n",
    "        df_share.loc[i,'Quarter'] = 2\n",
    "    if df_share.loc[i,'Month'] >=7 and df_share.loc[i,'Month']<10:\n",
    "        df_share.loc[i,'Quarter'] = 3\n",
    "    if df_share.loc[i,'Month'] >=10:\n",
    "        df_share.loc[i,'Quarter'] =4\n",
    "\n",
    "df_share.sort_values(by=['Year','Quarter'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging the sentinment scores with the spending data\n",
    "mean_scores.rename(columns={'year':'Year'},inplace=True)\n",
    "df = pd.merge(df_share,mean_scores,on=['Year','Quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a 2 quarter and a one year lag, two year lag \n",
    "\n",
    "df.sort_values(by=['Year','Quarter'],inplace=True)\n",
    "# 2 quarter lag \n",
    "df['Spending (quarter + 2)'] = df['National Defense spending (share of GDP)'].shift(-2)\n",
    "# One year lag \n",
    "df['Spending (year +1)'] = df['National Defense spending (share of GDP)'].shift(-4)\n",
    "# 2 years lag \n",
    "df['Spending (year +2)'] = df['National Defense spending (share of GDP)'].shift(-8)\n",
    "\n",
    "# # delete last two quarters of observations \n",
    "df = df.loc[df['Year']<2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS regressions using word count \n",
    "\n",
    "X = (df['Polarity_wordcount'] - df['Polarity_wordcount'].mean())/df['Polarity_wordcount'].std()\n",
    "y = (df['Spending (quarter + 2)'] - df['Spending (quarter + 2)'].mean()) / df['Spending (quarter + 2)'].std()\n",
    "\n",
    "\n",
    "\n",
    "df2 = df.dropna(subset=['Spending (year +1)'])\n",
    "X2 = (df2['Polarity_wordcount'] - df2['Polarity_wordcount'].mean())/df2['Polarity_wordcount'].std()\n",
    "y_2 = (df2['Spending (year +1)'] - df2['Spending (year +1)'].mean()) / df2['Spending (year +1)'].std()\n",
    "\n",
    "\n",
    "df3 = df2.dropna(subset=['Spending (year +2)'])\n",
    "X3= (df3['Polarity_wordcount'] - df3['Polarity_wordcount'].mean())/df3['Polarity_wordcount'].std()\n",
    "y_3 = (df3['Spending (year +2)'] - df3['Spending (year +2)'].mean()) / df3['Spending (year +2)'].std()\n",
    "\n",
    "# split into training and testing dataset\n",
    "X = sm.add_constant(X)\n",
    "X2 = sm.add_constant(X2)\n",
    "X3 = sm.add_constant(X3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "X2_train , X2_test, y2_train, y2_test = train_test_split(X2, y_2, test_size=0.1, random_state=0)\n",
    "X3_train , X3_test, y3_train, y3_test = train_test_split(X3, y_3, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run ols regressions with a 2 quarter lag \n",
    "\n",
    "ols = sm.OLS(y_train,X_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores \n",
    "y_pred = olsReg.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test)**2)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ols regressions with a year lag \n",
    "\n",
    "ols = sm.OLS(y2_train,X2_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ols regressions with a two year lag \n",
    "\n",
    "ols = sm.OLS(y3_train,X3_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores \n",
    "y_pred = olsReg.predict(X2_test)\n",
    "mse = np.mean((y_pred - y2_test)**2)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging second sentinment score with spending data \n",
    "\n",
    "mean_score2.rename(columns={'year':'Year'},inplace=True)\n",
    "df2 = pd.merge(df_share,mean_score2,on=['Year','Quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a 2 quarter and a one year lag \n",
    "\n",
    "df2.sort_values(by=['Year','Quarter'],inplace=True)\n",
    "df2['Spending (quarter + 2)'] = df2['National Defense spending (share of GDP)'].shift(-2)\n",
    "df2['Spending (year +1)'] = df2['National Defense spending (share of GDP)'].shift(-4)\n",
    "# 2 years lag \n",
    "df2['Spending (year +2)'] = df2['National Defense spending (share of GDP)'].shift(-8)\n",
    "\n",
    "\n",
    "\n",
    "# delete last two quarters of observations \n",
    "df2 = df2.loc[df2['Year']<2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS with TFIDF\n",
    "# standardizing data\n",
    "\n",
    "\n",
    "X = (df2['Polarity_TFIDF'] - df2['Polarity_TFIDF'].mean())/df2['Polarity_TFIDF'].std()\n",
    "y = (df2['Spending (quarter + 2)'] - df2['Spending (quarter + 2)'].mean()) / df2['Spending (quarter + 2)'].std()\n",
    "\n",
    "\n",
    "\n",
    "df2 = df2.dropna(subset=['Spending (year +1)'])\n",
    "X2 = (df2['Polarity_TFIDF'] - df2['Polarity_TFIDF'].mean())/df2['Polarity_TFIDF'].std()\n",
    "y_2 = (df2['Spending (year +1)'] - df2['Spending (year +1)'].mean()) / df2['Spending (year +1)'].std()\n",
    "\n",
    "\n",
    "df3 = df2.dropna(subset=['Spending (year +2)'])\n",
    "X3 = (df3['Polarity_TFIDF'] - df3['Polarity_TFIDF'].mean())/df3['Polarity_TFIDF'].std()\n",
    "y_3 = (df3['Spending (year +2)'] - df3['Spending (year +2)'].mean()) / df3['Spending (year +2)'].std()\n",
    "\n",
    "# split into training and testing dataset\n",
    "X = sm.add_constant(X)\n",
    "X2 = sm.add_constant(X2)\n",
    "X3 = sm.add_constant(X3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "X2_train , X2_test, y2_train, y2_test = train_test_split(X2, y_2, test_size=0.1, random_state=0)\n",
    "X3_train , X3_test, y3_train, y3_test = train_test_split(X3, y_3, test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run ols regressions with a 2 quarter lag \n",
    "\n",
    "ols = sm.OLS(y_train,X_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ols regressions with a year lag \n",
    "\n",
    "ols = sm.OLS(y2_train,X2_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ols regressions with a two year lag \n",
    "\n",
    "ols = sm.OLS(y3_train,X3_train)\n",
    "olsReg = ols.fit()\n",
    "olsReg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores \n",
    "y_pred = olsReg.predict(X_test)\n",
    "mse = np.mean((y_pred - y_test)**2)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores \n",
    "y_pred = olsReg.predict(X2_test)\n",
    "mse = np.mean((y_pred - y2_test)**2)\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between word count using vectors and sentinment anlaysis package \n",
    "\n",
    "df_news['dif'] = 0\n",
    "for i in range (len(df_news)):\n",
    "    df_news.loc[i,'dif'] = np.abs(df_news.loc[i,'Polarity_wordcount'] - df_news.loc[i,'score'])\n",
    "\n",
    "df_news.sort_values(by='dif',inplace=True,ascending=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
